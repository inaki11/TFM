{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/inaki/host_data')\n",
    "import models\n",
    "from utils import train_loop, evaluate, evaluate_kfold_ensemble\n",
    "from CreateDataset import BertDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#       configuración         #\n",
    "###############################\n",
    "MODEL_NAME = \"BertBaseDenseLogits\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "HEAD_DROPOUT = 0.0\n",
    "DATA_AUGMENTATION = [] #[\"Oversampling\"] # '_es_into_en' # 'llama_aug'\n",
    "FOLDS_NUM = 10\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "DECAY = 0\n",
    "\n",
    "LOSS_FN =  'supervised_contrastive' # 'cross_entropy' # 'supervised_contrastive'\n",
    "\n",
    "temperature = 0.3  # temprature for contrastive loss\n",
    "lam = 0.9  # lambda for loss\n",
    "\n",
    "checkoint_folder = 'Baseline'\n",
    "\n",
    "datadir = \"/home/inaki/host_data/dataset_oppositional/\"\n",
    "cuda_device = 0\n",
    "wandb_project = 'epoch_analysis'    # 'BASELINE'  'Ensemble_baseline'  'trash'  'epoch_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 17:23:46 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:18:00.0 Off |                  N/A |\n",
      "| 30%   35C    P8              29W / 350W |    780MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 30%   35C    P8              23W / 350W |   1603MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        Off | 00000000:99:00.0 Off |                  N/A |\n",
      "| 70%   67C    P2             346W / 350W |  20761MiB / 24576MiB |     97%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cargamos los datasets de entrenamiento y test\n",
    "train_en_dataset_path = datadir + \"train_en_data.pth\"\n",
    "X, y = torch.load(train_en_dataset_path)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "train_en_dataset = BertDataset(X, y)\n",
    "\n",
    "# Obtén la referencia a la clase del módulo models\n",
    "ModelClass = getattr(models, MODEL_NAME)\n",
    "model = ModelClass(dropout_prob=HEAD_DROPOUT, dense_dim=32)\n",
    "\n",
    "device = torch.device(f\"cuda:{cuda_device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19.1925],\n",
      "        [18.4902],\n",
      "        [15.5536],\n",
      "        [18.8394],\n",
      "        [19.6545]], device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor([[1.0000, 0.9836, 0.9393, 0.9550, 0.9926],\n",
      "        [0.9836, 1.0000, 0.9403, 0.9510, 0.9715],\n",
      "        [0.9393, 0.9403, 1.0000, 0.9334, 0.9195],\n",
      "        [0.9550, 0.9510, 0.9334, 1.0000, 0.9498],\n",
      "        [0.9926, 0.9715, 0.9195, 0.9498, 1.0000]], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n",
      "tensor([[ 1.0000, 26.5409, 22.9002, 24.1306, 27.3483],\n",
      "        [26.5409,  1.0000, 22.9765, 23.8062, 25.4928],\n",
      "        [22.9002, 22.9765,  1.0000, 22.4483, 21.4311],\n",
      "        [24.1306, 23.8062, 22.4483,  1.0000, 23.7140],\n",
      "        [27.3483, 25.4928, 21.4311, 23.7140,  1.0000]], device='cuda:0',\n",
      "       grad_fn=<ExpBackward0>)\n",
      "tensor([101.9200,  99.8164,  90.7561,  95.0991,  98.9863], device='cuda:0',\n",
      "       grad_fn=<SumBackward1>)\n",
      "tensor([0, 1, 0, 0, 0], device='cuda:0')\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    # calculate inner sum\\n    for j in range(len(embedding)):\\n        if label[i] == label[j] and i != j:\\n            inner_sum = inner_sum + torch.log(cosine_sim[i][j] / row_sum[i])\\n    if n_i != 0:\\n        contrastive_loss += (inner_sum / (-n_i))\\n    else:\\n        contrastive_loss += 0\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp = 0.3\n",
    "dataloader = DataLoader(train_en_dataset, batch_size=5, shuffle=False)\n",
    "input_ids, attention_mask, labels, _ = dataloader.__iter__().__next__()\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "embedding, logit = model(input_ids, attention_mask)\n",
    "\n",
    "norm_L2 = torch.norm(embedding, p=2, dim=1, keepdim=True)\n",
    "print(norm_L2)\n",
    "\n",
    "# cosine similarity between embeddings\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(embedding.unsqueeze(1), embedding.unsqueeze(0), dim=2)\n",
    "print(cosine_sim)\n",
    "\n",
    "\n",
    "#print(dis)\n",
    "\n",
    "# apply temperature to elements\n",
    "dis = dis / temp\n",
    "#print(dis)\n",
    "cosine_sim = cosine_sim / temp\n",
    "\n",
    "# apply exp to elements\n",
    "dis = torch.exp(dis)\n",
    "print(dis)\n",
    "cosine_sim = torch.exp(cosine_sim)\n",
    "# calculate row sum\n",
    "row_sum = dis.sum(dim=1)\n",
    "print(row_sum)\n",
    "\n",
    "# calculate outer sum\n",
    "contrastive_loss = 0\n",
    "print(labels)\n",
    "for i in range(len(embedding)):\n",
    "    n_i = (labels == labels[i]).sum().item() - 1\n",
    "    print(n_i)\n",
    "    inner_sum = 0\n",
    "\"\"\"\n",
    "    # calculate inner sum\n",
    "    for j in range(len(embedding)):\n",
    "        if label[i] == label[j] and i != j:\n",
    "            inner_sum = inner_sum + torch.log(cosine_sim[i][j] / row_sum[i])\n",
    "    if n_i != 0:\n",
    "        contrastive_loss += (inner_sum / (-n_i))\n",
    "    else:\n",
    "        contrastive_loss += 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
