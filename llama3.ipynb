{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.37.0 -q\n",
    "%pip install -U accelerate bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 30 12:25:54 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:18:00.0 Off |                  N/A |\n",
      "| 65%   65C    P2             342W / 350W |   4587MiB / 24576MiB |     96%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 30%   32C    P8              23W / 350W |    776MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        Off | 00000000:99:00.0 Off |                  N/A |\n",
      "| 71%   68C    P2             344W / 350W |   1450MiB / 24576MiB |     86%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check if there are gpu available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inaki/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1085: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/inaki/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9836c18b71bb4340a11773b2ef20d938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inaki/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HF_TOKEN = \"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\"\n",
    "CACHE_DIR = \"/home/inaki/.cache/huggingface/hub\"\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "    # initialize the model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    #return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probamos a traducir una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inaki/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 3 µs, total: 14 µs\n",
      "Wall time: 31 µs\n",
      "My name is Iñaki and my favorite food is seafood paella.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a professional translator who translates phrases from Spanish to English on demand. You only answere the translation without any greetings and farewells.\"},\n",
    "    {\"role\": \"user\", \"content\": \"me llamo iñaki y mi comida favorita son los arroces de marisco\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probamos a pedirle una variación de la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Ignatius, but my friends call me Iñaki, and my gastronomic weakness is undoubtedly a succulent serving of seafood paella.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional writer who provides a modification of the given sentence on demand. It is very important to keep the general meaning of the sentence, so in general only words or expressions should be replaced by synonyms, the more changes the better. Additionally, it is also advisable to change the style of writing. It is very important that You only respond the modified sentence, without any greetings and farewells.\"},\n",
    "    {\"role\": \"user\", \"content\": \"My name is Iñaki and my favorite food is seafood paella.\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hacemos data augmentation con el dataset original en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3600 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 1/3600 [00:06<6:55:07,  6.92s/it, estimated time remaining=24914.18 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 2/3600 [00:36<20:04:42, 20.09s/it, estimated time remaining=65178.15 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 3/3600 [00:44<14:27:12, 14.47s/it, estimated time remaining=52760.21 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 4/3600 [00:52<12:13:12, 12.23s/it, estimated time remaining=47481.51 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 5/3600 [00:59<10:10:49, 10.19s/it, estimated time remaining=42704.94 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 6/3600 [01:05<8:39:06,  8.67s/it, estimated time remaining=38991.51 seconds] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 7/3600 [01:17<9:59:33, 10.01s/it, estimated time remaining=39973.24 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 8/3600 [01:27<9:58:01,  9.99s/it, estimated time remaining=39430.24 seconds]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "# load dataset train_es_data.pth\n",
    "datadir = \"/home/inaki/host_data/dataset_oppositional/\"\n",
    "train_en_dataset_path = datadir + \"train_en_data.pth\"\n",
    "train_en_dataset = torch.load(train_en_dataset_path)\n",
    "system_prompt = \"You are a professional writer who provides a modification of the given sentence on demand. It is very important to keep the general meaning of the sentence, so in general only words or expressions should be replaced by synonyms, the more changes the better. Additionally, it is also advisable to change the style of writing. It is very important that You only respond the modified sentence, without any greetings and farewells.\"\n",
    "variations_number = 2\n",
    "\n",
    "# iterate over the dataset and translate the spanish sentences to english\n",
    "augmented = []\n",
    "labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = tqdm.tqdm(range(len(train_en_dataset[0])))\n",
    "for i in pbar:\n",
    "    for j in range(variations_number):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": train_en_dataset[0][i]},\n",
    "        ]\n",
    "\n",
    "        prompt = pipeline.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        terminators = [\n",
    "            pipeline.tokenizer.eos_token_id,\n",
    "            pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        outputs = pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        # print(train_en_dataset[0][i])\n",
    "        # print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "        # print(\"--------------------\")\n",
    "        augmented += [outputs[0][\"generated_text\"][len(prompt):]]\n",
    "        labels += [train_en_dataset[1][i]]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    remaining_time = elapsed_time / (i+1) * (len(train_en_dataset[0]) - (i+1))\n",
    "    pbar.set_postfix({'estimated time remaining': f'{remaining_time:.2f} seconds'})\n",
    "\n",
    "# save new dataset\n",
    "new_dataset_path = datadir + f\"train_en_data_AUG_1-{variations_number}.pth\"\n",
    "torch.save([augmented, labels], new_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traducimos el dataset español a ingles y lo guardamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3600 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 0/3600 [08:54<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El G7 realiza una simulación de “ Pandemia de Viruela de leopardo ” : los ministros de salud se reúnen “ contra la próxima crisis ” Los ministros de salud del G7 ya están practicando “ la próxima pandemia ” . En Berlín , realizan una simulación del curso de una pandemia de viruela en 2023 , según informa el periódico alemán Bild . Cabe recordar que el mismo escenario futuro , en base esta vez a una modificación de la viruela , viene siendo anunciado por el magnate Bill Gates .   | https :// trikooba . blog / 45408 . html   Síguenos en :   Nuevo TELEGRAM : https :// t . me / trikooba2022   Nuevo FACEBOOK : https :// bit . ly / 38b1CEr INSTAGRAM : instagram . com / trikooba TWITTER : twitter . com / 3Kooba _ com MEWE : bit . ly / 3dxxenE VK : vk . com / trikoobanews \n",
      "The G7 is conducting a simulation of a \"Leopard Virus Pandemic\": health ministers meet \"against the next crisis\". The G7 health ministers are already practicing \"the next pandemic\". In Berlin, they are conducting a simulation of the course of a 2023 smallpox pandemic, according to the German Bild newspaper. It is worth noting that the same future scenario, based on a modification of smallpox, has been announced by billionaire Bill Gates.\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# load dataset train_es_data.pth\n",
    "datadir = \"/home/iñaki/host_data/dataset_oppositional/\"\n",
    "train_es_dataset_path = datadir + \"train_es_data.pth\"\n",
    "train_es_dataset = torch.load(train_es_dataset_path)\n",
    "\n",
    "# iterate over the dataset and translate the spanish sentences to english\n",
    "translated = []\n",
    "for i in tqdm.tqdm(range(len(train_es_dataset[0]))):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"you are a professional translator who translates phrases from Spanish to English on demand. You only answere the translation without any greetings and farewells.\"},\n",
    "        {\"role\": \"user\", \"content\": train_es_dataset[0][i]},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(train_es_dataset[0][i])\n",
    "    print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "    print(\"--------------------\")\n",
    "    translated += [outputs[0][\"generated_text\"][len(prompt):]]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new dataset\n",
    "train_es_translated_data_path = datadir + \"train_es-translated_data.pth\"\n",
    "torch.save([translated, train_es_dataset[1]], train_es_translated_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
