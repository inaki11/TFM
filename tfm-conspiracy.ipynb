{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-04T22:08:09.883027Z","iopub.status.busy":"2024-04-04T22:08:09.882671Z","iopub.status.idle":"2024-04-04T22:08:18.639334Z","shell.execute_reply":"2024-04-04T22:08:18.638457Z","shell.execute_reply.started":"2024-04-04T22:08:09.882996Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n","\n","import numpy as np\n","import pandas as pd \n","import json\n","\n","import matplotlib.pyplot as plt\n","from transformers import BertModel\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from transformers import BertTokenizer, get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n","from sklearn.metrics import matthews_corrcoef, confusion_matrix\n","\n","import random\n","import wandb\n","#%pip install deepl\n","import deepl\n","\n","import sys\n","sys.path.append('/home/iñaki/host_data')\n","from models import BertBasePooledOutput\n","from CreateDataset import BertDataset, createAuxDeepLDataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T22:08:18.659675Z","iopub.status.busy":"2024-04-04T22:08:18.659387Z","iopub.status.idle":"2024-04-04T22:08:18.666414Z","shell.execute_reply":"2024-04-04T22:08:18.665390Z","shell.execute_reply.started":"2024-04-04T22:08:18.659643Z"},"trusted":true},"outputs":[],"source":["########################################\n","#                Ideas                 #\n","########################################\n","\n","# Data Augmentation\n","    # Crear x10 el numero de elementos del dataset mediante LLMs\n","    # Una idea es utilizar como propmpt dos o tres textos aleatorios originales etiquetados igual y probar a generar un nuevo texto.\n","        # Se podria aletorizar tambien el propmt para variar los estilos de escritura. \n","        #        (Persona de nivel cultural bajo, Entendido, Pretencioso, Misterioso, paranoico, asetivo, profesor) \n","        #     Adjetivos que definen el estilo  \n","        # Intrincado, Descriptivo, Cautivador, Persuasivo, Elegante, Intenso, Imaginativo, Sobrio, Oscuro, Profundo, Emotivo, \n","        # Surrealista, Irreverente, Oscilante, Contundente, Desgarrador, Espeluznante, Enigmático, Inquietante, Mordaz\"\"\"\n","        # Se podría entrenar inicialmente con estos datos, y finalmente con los originales.  \n","        # Finalmente testear con test, que no ha sido usado para generar prompts\n","        \n","    # Otros metodos  -  https://neptune.ai/blog/data-augmentation-nlp\n","        # Back translation\n","        # Easy methods\n","        #     Synonym Replacement  |  Random synonym Insertion   |   Random Swap   |    Random Deletion\n","        # Albumentation (libreria)\n","        #     Shuffle Sentences Transform \n","        # Libreria para muchos de estos métodos  -  NLPAug \n","        \n","    # Aumentado sobre embedding\n","        # Mezclar 2 embeddings\n","        # Añadir ruido gaussiano\n","        # Ruido adversial\n","        # MixUp  (mezclar embedding y su label en la proporcion dada por una funcion)\n","        \n","    # Consistency regularization ??\n","    \n","    # Contrastive learning ??\n","\n","# TRADUCIR EL DATASET ESPAÑOL A INGLES Y PROBAR\n","\n","# DUDAS\n","    # Guardo la mejor epoch?\n","\n","# TO DO tutorias\n","    # Guardar logits baseline para probar \n","    # Guardar modelos para probar forma de ensembles\n","    # repetir enseble 10 veces (baja sigma a la mitad)\n","    # Tecnicas basicas de data aumentacion\n","    # Baseline 10 folds sobre test, para tener 10 test como al probar 10 ensembles\n","# ----------\n","     \n","    # Analizar matriz de confusion sobre baseline\n","    \n","    # Guardar las predicciones de los k-modelos\n","    \n","    # Bert ajustado para mensajes de redes sociales (telegram)\n","    \n","# Recently DONE\n","    # Plotear distribucion de errores\n","    # implementar Predicciones 0, 1 o no lo se\n","    # Matriz de confusion\n","    # probar los k modelos sobre test\n","    # Guardar ultimo epoch de validacion, quitar MCC cada epoch, gaurdar media y desviacion del ultimo\n","    \n","    # Mirar si tiene emojis y traducirlos a texto (emojify)\n","    \n","# DONE\n","    #  ¿¿¿ #os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\" ???\n","       # ERROR  # torch.use_deterministic_algorithms(True)  \n","    # guardar los resultados weight and bias o ML flow\n","    # Intentar guardar el MCC de cada fold para poder sacar desviaciones\n","    # Guardar semilas.  La semilla es para que los datos sean los mismos.\n","    # MCC  poner etiquetas a 1 y 2  ¿¿¿ seguro ???\n","    # Arreglar docker\n","    # sacar 10% de datos del dataset como sanity-check \n","    # split stratify   (sklearn viene)\n","    # cross validation (sklearn)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:02:56.153864Z","iopub.status.busy":"2024-04-04T23:02:56.152955Z","iopub.status.idle":"2024-04-04T23:02:56.159239Z","shell.execute_reply":"2024-04-04T23:02:56.158120Z","shell.execute_reply.started":"2024-04-04T23:02:56.153830Z"},"trusted":true},"outputs":[],"source":["###############################\n","#       configuración         #\n","###############################\n","MODEL_NAME = \"BertBasePooledOutput\"\n","\n","MAX_LENGTH = 512\n","HEAD_DROPOUT = 0\n","DATA_AUGMENTATION = [\"Oversampling\"]\n","FOLDS_NUM = 10\n","\n","NUM_EPOCHS = 5\n","LEARNING_RATE = 2e-5\n","BATCH_SIZE = 16\n","\n","FREEZE_BACKBONE = True\n","FREEZE_BACKBONE_EPOCHS = 7\n","\n","\n","datadir = \"/home/iñaki/host_data/dataset_oppositional/\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:02:58.460893Z","iopub.status.busy":"2024-04-04T23:02:58.460599Z","iopub.status.idle":"2024-04-04T23:02:59.150899Z","shell.execute_reply":"2024-04-04T23:02:59.149887Z","shell.execute_reply.started":"2024-04-04T23:02:58.460871Z"},"trusted":true},"outputs":[],"source":["# Cargamos los datasets de entrenamiento y test\n","train_en_dataset_path = datadir + \"train_en_data.pth\"\n","test_en_dataset_path = datadir + \"test_en_data.pth\"\n","train_es_dataset_path = datadir + \"train_es_data.pth\"\n","test_es_dataset_path = datadir + \"test_es_data.pth\"\n","\n","# creamos los datasets de test\n","X, y = torch.load(test_es_dataset_path)\n","test_es_dataset = BertDataset(X, y)\n","X, y = torch.load(test_en_dataset_path)\n","test_en_dataset = BertDataset(X, y)\n","\n","#Creamos dataloader de test\n","test_en_dataloader = DataLoader(test_en_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:03:03.570563Z","iopub.status.busy":"2024-04-04T23:03:03.570189Z","iopub.status.idle":"2024-04-04T23:03:03.582061Z","shell.execute_reply":"2024-04-04T23:03:03.579468Z","shell.execute_reply.started":"2024-04-04T23:03:03.570532Z"},"trusted":true},"outputs":[],"source":["def train_loop(model, train_dataloader, optimizer, scheduler, NUM_EPOCHS):\n","    train_losses = []\n","    for epoch in range(NUM_EPOCHS):\n","        # Set your model to training mode\n","        total_loss = 0\n","        total_train_samples = 0\n","        model.train()\n","        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", unit=\"batch\") as tepoch:\n","            for i, data in enumerate(tepoch):\n","                input_ids, attention_mask, labels, _ = data\n","                # Move batch to device\n","                input_ids = input_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass\n","                optimizer.zero_grad()\n","                outputs = model(input_ids, attention_mask)\n","                logits = outputs.squeeze(-1)\n","\n","                # Compute loss\n","                loss = criterion(logits, labels.float())\n","\n","                # Backward pass\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","\n","                # Update tqdm description with loss\n","                tepoch.set_postfix(loss=loss.item())\n","                \n","                #update learning rate\n","                if scheduler:\n","                    scheduler.step()\n","\n","            # Print average loss for this epoch\n","            avg_train_loss = total_loss / len(train_dataloader)\n","            print(f\"Epoch {epoch+1}, Average Train Loss: {avg_train_loss}\")\n","            train_losses += [avg_train_loss]\n","            \n","    return train_losses"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:03:13.357811Z","iopub.status.busy":"2024-04-04T23:03:13.357392Z","iopub.status.idle":"2024-04-04T23:03:13.370432Z","shell.execute_reply":"2024-04-04T23:03:13.369266Z","shell.execute_reply.started":"2024-04-04T23:03:13.357780Z"},"trusted":true},"outputs":[],"source":["def evaluate(model, test_dataloader, THRESHOLD=0.5, LOWER_UPPER_BOUND=False, val_or_test=None):\n","    test_outputs = []\n","    test_true_labels = []\n","    with torch.no_grad():\n","        total_test_loss = 0\n","        total_test_samples = 0\n","        for data in test_dataloader:\n","            input_ids, attention_mask, labels, _ = data\n","            # Save list of ground truth labels for metrics\n","            test_true_labels += labels.tolist()\n","                    \n","            # Move batch to device\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","                    \n","            # Forward pass\n","            outputs = model(input_ids, attention_mask)\n","            test_outputs += outputs.tolist()\n","            logits = outputs.squeeze(-1)\n","\n","            # Compute loss\n","            test_loss = criterion(logits, labels.float())\n","\n","            # Accumulate test loss and total number of samples\n","            total_test_loss += test_loss.item() * labels.size(0)\n","            total_test_samples += labels.size(0)\n","            \n","    # Calculate average test loss\n","    average_test_loss = total_test_loss / total_test_samples   \n","                    \n","    test_predictions = [1 if x[0] > THRESHOLD else 0 for x in test_outputs]\n","    mcc = matthews_corrcoef(test_true_labels, test_predictions)\n","    print(f\"{val_or_test} MCC {THRESHOLD} threshold:  {mcc:.4f}\")\n","    \n","    # log in wandb\n","    if wandb.run is not None:\n","        wandb.log({f\"{val_or_test}_{THRESHOLD}_threshold_MCC\":mcc})\n","\n","    ##### matriz de confusion\n","    cm = confusion_matrix(test_true_labels, test_predictions)\n","    print(cm)\n","\n","    #### get index of errors in predictions\n","    errors = [i for i, x in enumerate(test_predictions) if x != test_true_labels[i]]\n","    #### get the model outputs for the errors\n","    errors_outputs = [test_outputs[i] for i in errors]\n","    #### plot the errors with title\n","    plt.hist([x[0] for x in errors_outputs]) \n","    plt.title(f\"errors distribution\")\n","    plt.show()\n","\n","    \n","    # Si hay valores para el rango de valor desconocido sobre-escribimos las predicciones  \n","    if LOWER_UPPER_BOUND != False:\n","        test_predictions = [1 if x[0] > LOWER_UPPER_BOUND[1] else (0 if x[0] <= LOWER_UPPER_BOUND[0] else -1) for x in test_outputs]\n","        mcc = matthews_corrcoef(test_true_labels, test_predictions)\n","        print(f\"{val_or_test} MCC with Lower_bound:{LOWER_UPPER_BOUND[0]} and upper_bound:{LOWER_UPPER_BOUND[1]} > MCC: {mcc:.4f}\")\n","        # log in wandb\n","        if wandb.run is not None:\n","            wandb.log({f\"{val_or_test}_lower_upper_{LOWER_UPPER_BOUND[0]}-{LOWER_UPPER_BOUND[1]}_MCC\":mcc})\n","            \n","    # Print average test loss for this epoch\n","    print(f\"Average {val_or_test} Loss: {average_test_loss:.4f}\")\n","    \n","    return test_outputs"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:11:26.260552Z","iopub.status.busy":"2024-04-04T23:11:26.259914Z","iopub.status.idle":"2024-04-04T23:11:26.267713Z","shell.execute_reply":"2024-04-04T23:11:26.266780Z","shell.execute_reply.started":"2024-04-04T23:11:26.260521Z"},"trusted":true},"outputs":[],"source":["def evaluate_kfold_ensemble(predictions, test_dataloader, THRESHOLD=0.5, LOWER_UPPER_BOUND=False):\n","    test_true_labels = []\n","    with torch.no_grad():\n","        for data in test_dataloader:\n","            input_ids, attention_mask, labels, _ = data\n","            # Save list of ground truth labels for metrics\n","            test_true_labels += labels.tolist()\n","            \n","    # Mean output\n","    test_outputs = np.array(predictions)\n","    print(test_outputs.shape)\n","    test_outputs = test_outputs.mean(axis=0)\n","    print(test_outputs.shape)\n","    test_mean_predictions = [1 if x > THRESHOLD else 0 for x in test_outputs]\n","    mcc = matthews_corrcoef(test_true_labels, test_mean_predictions)\n","    print(f\"test ensemble MCC {THRESHOLD} threshold: {mcc:.4f}\")\n","    # log in wandb\n","    if wandb.run is not None:\n","        wandb.log({f\"test_ensemble_{THRESHOLD}_threshold_MCC\":mcc})\n","    # Max Voting"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T23:11:26.540949Z","iopub.status.busy":"2024-04-04T23:11:26.540570Z","iopub.status.idle":"2024-04-04T23:11:29.224028Z","shell.execute_reply":"2024-04-04T23:11:29.222265Z","shell.execute_reply.started":"2024-04-04T23:11:26.540917Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["experimento 0:\n","seed: 2635\n"]},{"name":"stdout","output_type":"stream","text":["fold 1\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33minakitodc\u001b[0m (\u001b[33minaki\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.16.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/inaki/wandb/run-20240417_111948-p25pcn63</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/inaki/BASELINE/runs/p25pcn63' target=\"_blank\">light-cosmos-4</a></strong> to <a href='https://wandb.ai/inaki/BASELINE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/inaki/BASELINE' target=\"_blank\">https://wandb.ai/inaki/BASELINE</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/inaki/BASELINE/runs/p25pcn63' target=\"_blank\">https://wandb.ai/inaki/BASELINE/runs/p25pcn63</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["cuda\n","total params: 109483009 trainble params: 109483009\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/5:   0%|          | 1/203 [00:05<18:46,  5.58s/batch, loss=0.665]\n"]},{"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/iñaki/host_data/models.py\", line 15, in forward\n    outputs = self.bert(input_ids, attention_mask=attention_mask)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1022, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 612, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n    layer_output = apply_chunking_to_forward(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 239, in apply_chunking_to_forward\n    return forward_fn(*input_tensors)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 552, in feed_forward_chunk\n    layer_output = self.output(intermediate_output, attention_output)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 466, in forward\n    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 190, in forward\n    return F.layer_norm(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2515, in layer_norm\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.77 GiB total capacity; 5.82 GiB already allocated; 8.25 MiB free; 6.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(total_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     95\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps, num_training_steps\u001b[38;5;241m=\u001b[39mtotal_steps)\n\u001b[0;32m---> 97\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_en_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     99\u001b[0m _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_en_dataloader, THRESHOLD\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, val_or_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m k_test_outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [evaluate(model, test_en_dataloader, THRESHOLD\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, LOWER_UPPER_BOUND\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.6\u001b[39m], val_or_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n","Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_dataloader, optimizer, scheduler, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/iñaki/host_data/models.py\", line 15, in forward\n    outputs = self.bert(input_ids, attention_mask=attention_mask)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 1022, in forward\n    encoder_outputs = self.encoder(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 612, in forward\n    layer_outputs = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 539, in forward\n    layer_output = apply_chunking_to_forward(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\", line 239, in apply_chunking_to_forward\n    return forward_fn(*input_tensors)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 552, in feed_forward_chunk\n    layer_output = self.output(intermediate_output, attention_output)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\", line 466, in forward\n    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 190, in forward\n    return F.layer_norm(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2515, in layer_norm\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.77 GiB total capacity; 5.82 GiB already allocated; 8.25 MiB free; 6.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"]}],"source":["##################################\n","#     Weigths and bias Login    #\n","##################################\n","# MY API KEY\n","#    240062b42b3367d962eecb38802f856b41245735\n","#wandb.login(key=\"240062b42b3367d962eecb38802f856b41245735\")\n","\n","\n","for i in range(1):\n","    print(f\"experimento {i}:\")\n","    #####################\n","    #  Reproducibilidad #\n","    #####################\n","    SEED = random.randint(0,10000)\n","    print(f\"seed: {SEED}\")\n","    # fuente  -  https://pytorch.org/docs/stable/notes/randomness.html\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","    torch.use_deterministic_algorithms(True)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","    ######################################\n","    #          K-FOLD TRAIN LOOP         #\n","    ######################################\n","\n","    # CROSS VALIDATION LOOP\n","    X, y = torch.load(train_en_dataset_path)\n","    X = np.array(X)\n","    y = np.array(y)\n","\n","    skf = StratifiedKFold(n_splits=FOLDS_NUM, shuffle=True, random_state=42)\n","\n","    k_test_outputs = []\n","    # Loop over the folds\n","    for fold_idx, (train_index, val_index) in enumerate(skf.split(X, y)):\n","        print(f\"fold {fold_idx + 1}\")\n","        \n","        # Weigths and bias init\n","        config = {\n","        \"MAX_LENGTH\": MAX_LENGTH,\n","        \"HEAD_DROPOUT\": HEAD_DROPOUT,\n","        \"DATA_AUGMENTATION\": DATA_AUGMENTATION,\n","        \"FOLDS_NUM\": FOLDS_NUM,\n","        \"NUM_EPOCHS\": NUM_EPOCHS,\n","        \"LEARNING_RATE\": LEARNING_RATE,\n","        \"BATCH_SIZE\": BATCH_SIZE,\n","        \"FOLD_IDX\": fold_idx,\n","        \"SEED\": SEED\n","        }\n","        GROUP = f\"{HEAD_DROPOUT}_{BATCH_SIZE}_{LEARNING_RATE}\"\n","        wandb.init(project=\"BASELINE\", group=f\"Group_{i}\", config=config)\n","        \n","        # Create the fold-specific train and validation sets\n","        X_train_fold, X_val_fold = X[train_index], X[val_index]\n","        y_train_fold, y_val_fold = y[train_index], y[val_index]\n","        \n","        train_en_dataset = BertDataset(X_train_fold, y_train_fold)\n","        val_en_dataset = BertDataset(X_val_fold, y_val_fold)\n","        \n","        # Create the train dataloader with a random oversampling of the minority class\n","        if \"Oversampling\" in DATA_AUGMENTATION:\n","            # Oversampling Train Dataloader\n","            labels = [sample[2] for sample in train_en_dataset]\n","            class_counts = torch.bincount(torch.tensor(labels))\n","            class_weights = 1.0 / class_counts.float()\n","\n","            weights = [class_weights[label] for label in labels]\n","            sampler = WeightedRandomSampler(weights=weights, num_samples=len(train_en_dataset), replacement=True)\n","            train_en_dataloader = DataLoader(train_en_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n","            print(f\"Oversampling\")\n","        else:\n","            print(f\"No Oversampling\")\n","            train_en_dataloader = DataLoader(train_en_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","            \n","        val_en_dataloader = DataLoader(val_en_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","        \n","        # Instanciamos el modelo\n","        model = BertBasePooledOutput(dropout_prob=HEAD_DROPOUT)\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(device)\n","        model.to(device)\n","        \n","        # Paralelizamos el modelo si hay mas de una GPU\n","        if torch.cuda.device_count() > 1:\n","            model = nn.DataParallel(model, device_ids=[0, 1])\n","            \n","        # Define your loss function (binary cross-entropy in this case)\n","        criterion = nn.BCELoss()\n","          \n","        # Activamos todas las capas\n","        for param in model.parameters():\n","            param.requires_grad = True\n","        \n","        # Contmaos los parametros entrenables\n","        print(f\"total params: {sum(p.numel() for p in model.parameters())} trainble params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n","        \n","\n","        # Define your optimizer\n","        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","        \n","        # Define your learning rate scheduler\n","        total_steps = len(train_en_dataloader) * NUM_EPOCHS\n","        warmup_steps = int(total_steps * 0.1)\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n","\n","        train_losses = train_loop(model, train_en_dataloader, optimizer, scheduler, NUM_EPOCHS)   \n","        \n","        _ = evaluate(model, val_en_dataloader, THRESHOLD=0.5, val_or_test=\"Val\")\n","        k_test_outputs += [evaluate(model, test_en_dataloader, THRESHOLD=0.5, LOWER_UPPER_BOUND=[0.4, 0.6], val_or_test=\"Test\")]\n","        \n","        # Save the model\n","        torch.save(model.state_dict(), f\"/home/iñaki/host_data/checkpoints/Baseline/{MODEL_NAME}_fold_{fold_idx}.pth\")\n","    evaluate_kfold_ensemble(k_test_outputs, test_en_dataloader)\n","        \n","    # bB - Finalizamos el experimento\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#### Bucle cargando todos los modelos BASELINE\n","baseline_test_outputs = []\n","\n","for model_path in os.listdir(\"/home/iñaki/host_data/checkpoints/Baseline/\"):\n","    model = BertBasePooledOutput(dropout_prob=HEAD_DROPOUT)\n","    model.load_state_dict(torch.load(f\"/home/iñaki/host_data/checkpoints/Baseline/{model_path}\"))\n","    model.to(device)\n","    model.eval()\n","    baseline_test_outputs += [evaluate(model, test_en_dataloader, THRESHOLD=0.5, val_or_test=\"Test\")]\n","\n","\n","# Evaluamos el valor optimo para el threshold\n","\n","test_true_labels = []\n","for data in test_en_dataloader:\n","    input_ids, attention_mask, labels, _ = data\n","    test_true_labels += labels.tolist()\n","\n","test_true_labels\n","\n","all_mcc = []\n","threshold = []\n","for i in range(101):\n","    test_outputs = np.array(baseline_test_outputs)\n","    print(test_outputs.shape)\n","    test_outputs = test_outputs.mean(axis=0)\n","    print(test_outputs.shape)\n","    test_mean_predictions = [1 if x > i/100 else 0 for x in test_outputs]\n","    mcc = matthews_corrcoef(test_true_labels, test_mean_predictions)\n","    all_mcc += [mcc]\n","    threshold += [i/100]\n","\n","# plot a mcc vs threshold\n","plt.plot(all_mcc, threshold)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython.display import display_html\n","display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\", raw=True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4579360,"sourceId":7862518,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
